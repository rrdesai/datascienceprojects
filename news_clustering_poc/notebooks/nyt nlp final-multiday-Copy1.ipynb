{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "#!/usr/bin/python\n",
    "# parent = os.path.dirname(os.path.realpath(__file__))\n",
    "sys.path.append('~/mitie')\n",
    "\n",
    "from mitie import *\n",
    "from collections import defaultdict\n",
    "\n",
    "ner = named_entity_extractor('../../../mitie/mitie-v0.2-python-2.7-windows-or-linux64/MITIE-models/english/ner_model.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import matutils\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161102_times.json\t\t\t kmeans_test_4.csv\r\n",
      "161103_times.json\t\t\t kmeans_test.csv\r\n",
      "161105e_nyt.json\t\t\t Kojak_testing.ipynb\r\n",
      "161107e_nyt.json\t\t\t news\r\n",
      "161109_nytm2.json\t\t\t nytimes_extended.csv\r\n",
      "161109_nytm.json\t\t\t nytimes_test_vocab.pkl\r\n",
      "161110_nytm.json\t\t\t nyt nlp final.ipynb\r\n",
      "161111_nytm.json\t\t\t nyt nlp final-multiday-Copy1.ipynb\r\n",
      "161112_nytm.json\t\t\t nyt nlp final-multiday.ipynb\r\n",
      "161114_nytm.json\t\t\t parse_nyt_data-Copy1.ipynb\r\n",
      "161116_nytimes_extended_clusters_50.csv  parse_nyt_data.ipynb\r\n",
      "161116_nytimes_extended_clusters.csv\t scrapy.cfg\r\n",
      "161121nyt2.json\t\t\t\t tfidf_docsimilarity.ipynb\r\n",
      "161121nyt.json\t\t\t\t ward_clusters.png\r\n",
      "Clusters.png\t\t\t\t withspec2.csv\r\n",
      "kmeans_test_2.csv\t\t\t withspec.csv\r\n",
      "kmeans_test_3.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['161102_times.json','161103_times.json','161105e_nyt.json','161105e_nyt.json','161107e_nyt.json','161109_nytm2.json','161110_nytm.json','161114_nytm.json']\n",
    "splits = []\n",
    "for file_ in files:\n",
    "    f = open(file_,'r')\n",
    "    filestring = f.read()\n",
    "    split = filestring.replace(']\\n','').replace('[\\n','').split('\\n')\n",
    "    splits.extend(split)\n",
    "final_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40472\n"
     ]
    }
   ],
   "source": [
    "print len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in splits:\n",
    "    try:\n",
    "        temp = json.loads(i[:-1])\n",
    "    except:\n",
    "        continue   \n",
    "     \n",
    "    key = temp.keys()[0].split('?')[0]\n",
    "    if '?' in key:\n",
    "        print key\n",
    "        break\n",
    "    val = x.encode('utf-8').replace(\"\\xe2\\x80\\x99\",\"'\").replace(\"\\xe2\\x80\\x9c\",'\"').replace(\"\\xe2\\x80\\x9d\",'\"').replace('\\xe2\\x80\\x94',\"'\").replace('\\xe2\\x80\\xa6','...')\n",
    "       \n",
    "    if val != '':\n",
    "        final_dict[key] = val\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'http://www.nytimes.com/2014/01/10/world/middleeast/syrian-groups-try-to-recruit-us-travelers.html',\n",
       " u'http://www.nytimes.com/2014/03/19/sports/baseball/a-fast-growing-fraternity-in-the-game-tommy-john-patients.html',\n",
       " u'http://www.nytimes.com/2009/07/04/us/politics/04alaska.html',\n",
       " u'http://www.nytimes.com/2015/08/01/business/into-the-family-business-at-perdue.html',\n",
       " u'http://www.nytimes.com/2010/03/08/world/middleeast/08sunnis.html',\n",
       " u'http://www.nytimes.com/2009/06/17/world/middleeast/17tehran.html',\n",
       " u'http://www.nytimes.com/2015/04/25/opinion/laurent-fabius-our-climate-imperatives.html',\n",
       " u'http://www.nytimes.com/2015/02/17/opinion/yanis-varoufakis-no-time-for-games-in-europe.html',\n",
       " u'http://www.nytimes.com/2014/02/11/nyregion/de-blasio-state-of-the-city-speech.html',\n",
       " u'http://www.nytimes.com/2015/07/10/us/politics/jeb-bush-races-past-rivals-in-fund-raising-aided-by-super-pac-cash.html']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict.keys()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_dict = json.dumps(final_dict)\n",
    "df = pd.read_json(dump_dict, typ = 'series')\n",
    "df = pd.DataFrame(df).reset_index()\n",
    "df.columns  = [['url','text']]\n",
    "df['date'] = df['url'].apply(lambda x: x.split('.com/')[1].split('/')[:3])\n",
    "df['section'] = df['url'].apply(lambda x: x.split('.com/')[1].split('/')[3])\n",
    "df['length'] = df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155974508"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entitiy Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-38d44fcc8d26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'unicode'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv2 = CountVectorizer(stop_words='english',max_df = 0.2, min_df = 0.1, strip_accents = 'unicode',analyzer='word').fit(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2b006eb8e755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mentities_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcommon_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "def get_entities(x, cv=cv2):\n",
    "    x = tokenize(x)\n",
    "    entities = ner.extract_entities(x)\n",
    "    entities_list = []\n",
    "    common_words = cv.vocabulary_.keys()\n",
    "    for e in entities:\n",
    "        rangee = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(x[i] for i in rangee)\n",
    "        entities_list.append(entity_text)\n",
    "    for i in x:\n",
    "        if i.lower() in common_words:\n",
    "            entities_list.append(i)\n",
    "    return entities_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entities(x):\n",
    "    x = tokenize(x)\n",
    "    entities = ner.extract_entities(x)\n",
    "    entities_list = []\n",
    "    for e in entities:\n",
    "        rangee = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(x[i] for i in rangee)\n",
    "        entities_list.append(entity_text)\n",
    "    return entities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HONG KONG',\n",
       " 'Bishop Francis X. Ford',\n",
       " 'Brooklyn-born',\n",
       " 'Meihsien',\n",
       " 'Kwangtung Province',\n",
       " 'Canton',\n",
       " 'Chinese',\n",
       " 'Communist']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(df['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization for Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_me(x):\n",
    "    x = word_tokenize(x)\n",
    "    x_tagged = nltk.pos_tag(x)\n",
    "    return_list = []\n",
    "    for i in x_tagged:\n",
    "        if 'V' in i[1]:\n",
    "            value = 'v'\n",
    "        else:\n",
    "            value = 'n'\n",
    "        return_list.append(lemmatizer.lemmatize(i[0], pos = value))\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create countvectorizer object, transform text to sparse matrix using named entitiy processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer = get_entities,stop_words='english', max_df = 0.8, min_df = 0.01, strip_accents='unicode', ngram_range=(1,3)).fit(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm = cv2.transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit TF IDF and transform sparse matrix from count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = TfidfTransformer().fit_transform(sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(analyzer= get_entities, stop_words='english', max_df=0.20, min_df=0.005, strip_accents='unicode').fit(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = tv.transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trained on count vectorizer sparse matrix, better explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=20)\n",
    "nyt_lsa = lsa.fit_transform(sm)\n",
    "nyt_lsa = Normalizer(copy=False).fit_transform(nyt_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyt_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsa.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do the damn thing. Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7)\n",
    "km = kmeans.fit(tt)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['clusters'] = clusters\n",
    "df['clusters2'] = clusters2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['url','clusters','clusters2','spec']].to_csv('withspec2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans2 = KMeans(n_clusters=50)\n",
    "km2 = kmeans2.fit(tfidf)\n",
    "clusters2 = km2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral wtf is this shit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec = SpectralClustering(n_clusters=25, affinity='cosine', n_init=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_fit = spec.fit(tfidf)\n",
    "spec_pred = spec.fit_predict(tfidf)\n",
    "df['spec'] = spec_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "af_mat = pd.DataFrame(spec_fit.affinity_matrix_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "af_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print df[['url']][df['spec'] ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spec = SpectralClustering(n_clusters=50, eigen_solver='arpack', affinity='sigmoid') #good clustering for certain topics mosul(7??) comey, clinton, trump, india 12, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_fit.n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization of matrix block structure\n",
    "(eg, pairwise similarity or co-occurrence)\n",
    "Requires scaledimage.py for intensity plots\n",
    "David Andrzejewski\n",
    "\"\"\"\n",
    "import numpy as NP\n",
    "import matplotlib.pyplot as P\n",
    "import matplotlib.lines as L\n",
    "import sklearn.cluster as SKLC\n",
    "\n",
    "# from scaledimage import scaledimage\n",
    "\n",
    "def blockviz(affinity, nclust, ax=None):\n",
    "    \"\"\" \n",
    "    Visualize block-structure of affinity matrix \n",
    "    affinity = NxN non-negative affinity matrix\n",
    "    nclust = number of clusters to use\n",
    "    ax = matplotlib Axes to draw on \n",
    "    Rely on caller to .show()\n",
    "    \"\"\"\n",
    "    # Activate the appropriate Axes in pyplot\n",
    "    if(ax == None):\n",
    "        ax = P.figure().gca()\n",
    "    # Do spectral clustering\n",
    "    ndata = affinity.shape[0]\n",
    "    c = SKLC.SpectralClustering(n_clusters=nclust, affinity='linear', random_state=1445)\n",
    "    c.fit(affinity)\n",
    "    # Extract cluster labels and sort indices to align with clusters\n",
    "    sortidx = []\n",
    "    for ki in range(nclust):\n",
    "        sortidx += getlabeled(c.labels_, ki)\n",
    "    sorted_affinity = affinity.copy()\n",
    "    sorted_affinity = sorted_affinity[sortidx,:]\n",
    "    sorted_affinity = sorted_affinity[:,sortidx]\n",
    "    # Intensity plot of affinity\n",
    "    width = 10\n",
    "    scaledimage(sorted_affinity,\n",
    "                pixwidth=width, grayscale=True, ax=ax)\n",
    "    # Draw recovered cluster boundaries\n",
    "    kstart = 0\n",
    "    clus_size = []\n",
    "    clus_start = []\n",
    "    for ki in range(nclust):    \n",
    "        clustki = getlabeled(c.labels_, ki)\n",
    "        clus_size.append(clustki)\n",
    "        kstart += len(clustki)\n",
    "        clus_start.append(kstart)\n",
    "    return c.labels_\n",
    "\n",
    "def logistic(val):\n",
    "    \"\"\" Logistic function \"\"\"\n",
    "    return float(1) / (1 + NP.exp(-1 * val))\n",
    "\n",
    "def getlabeled(labels, ki):\n",
    "    \"\"\" Get indices where labels==ki \"\"\"\n",
    "    return [idx for (idx, val) in \n",
    "            enumerate(labels) if val == ki]\n",
    "\n",
    "def drawH(ax, y, xstart, xend):\n",
    "    \"\"\" Draw horiztonal line \"\"\"\n",
    "    ax.add_line(L.Line2D([xstart, xend], \n",
    "                  [y, y],\n",
    "                  color='r'))\n",
    "\n",
    "def drawV(ax, x, ystart, yend):\n",
    "    \"\"\" Draw vertical line \"\"\"\n",
    "    ax.add_line(L.Line2D([x, x],\n",
    "                         [ystart, yend], \n",
    "                         color='r'))\n",
    "\n",
    "def drawClust(ax, kstart, kend, kmax, scale=1):\n",
    "    \"\"\" Draw bounding box for cluster \"\"\"\n",
    "    skstart = scale * kstart\n",
    "    skend = scale * kend\n",
    "    skmax = scale * kmax\n",
    "    if(skstart == 0):\n",
    "        # Upper-left cluster: only draw bottom-right borders\n",
    "        drawH(ax, skmax-skend, skstart, skend)\n",
    "        drawV(ax, skend, skmax-skstart, skmax-skend)\n",
    "    elif(skend == skmax):\n",
    "        # Lower-right cluster: only draw top-left borders\n",
    "        drawH(ax, skmax-skstart, skstart, skend)\n",
    "        drawV(ax, skstart, skmax-skstart, skmax-skend)\n",
    "    else:\n",
    "        # Otherwise, draw all 4 borders\n",
    "        drawH(ax, skmax-skend, skstart, skend)\n",
    "        drawV(ax, skend, skmax-skstart, skmax-skend)\n",
    "        drawH(ax, skmax-skstart, skstart, skend)\n",
    "        drawV(ax, skstart, skmax-skstart, skmax-skend)\n",
    "        \n",
    "import numpy as NP\n",
    "import matplotlib.pyplot as P\n",
    "import matplotlib.ticker as MT\n",
    "import matplotlib.cm as CM\n",
    " \n",
    "def scaledimage(W, pixwidth=20, ax=None, grayscale=True):\n",
    "    \"\"\"\n",
    "    Do intensity plot, similar to MATLAB imagesc()\n",
    " \n",
    "    W = intensity matrix to visualize\n",
    "    pixwidth = size of each W element\n",
    "    ax = matplotlib Axes to draw on \n",
    "    grayscale = use grayscale color map\n",
    " \n",
    "    Rely on caller to .show()\n",
    "    \"\"\"\n",
    "    # N = rows, M = column\n",
    "    (N, M) = W.shape \n",
    "    # Need to create a new Axes?\n",
    "    if(ax == None):\n",
    "        ax = P.figure(figsize=(20,20)).gca()\n",
    "    # extents = Left Right Bottom Top\n",
    "    exts = (0, pixwidth * M, 0, pixwidth * N)\n",
    "    if(grayscale):\n",
    "        ax.imshow(W,\n",
    "                  interpolation='nearest',\n",
    "                  cmap=CM.gray,\n",
    "                  extent=exts)\n",
    "    else:\n",
    "        ax.imshow(W,\n",
    "                  interpolation='nearest',\n",
    "                  extent=exts)\n",
    " \n",
    "    ax.xaxis.set_major_locator(MT.NullLocator())\n",
    "    ax.yaxis.set_major_locator(MT.NullLocator())\n",
    "    return ax\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    # Define a synthetic test dataset\n",
    "     testweights = NP.array([[0.25, 0.50, 0.25, 0.00],\n",
    "                            [0.00, 0.50, 0.00, 0.00],\n",
    "                            [0.00, 0.10, 0.10, 0.00],\n",
    "                            [0.00, 0.00, 0.25, 0.75]])\n",
    "    # Display it\n",
    "#     ax = scaledimage(testweights)\n",
    "#     P.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  cosine, 25 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blockviz(np.matrix(af_mat), 25, ax=P.figure(figsize=(50,50)).gca())\n",
    "P.savefig('Clusters.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cosine 30^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blockviz(np.matrix(af_mat), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cosine 20 ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blockviz(np.matrix(af_mat), 25, ax=P.figure(figsize=(100,100)).gca())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cosine 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blockviz(np.matrix(af_mat), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### poly 50 ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blockviz(np.matrix(af_mat), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### poly 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blockviz(np.matrix(af_mat), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poly 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
